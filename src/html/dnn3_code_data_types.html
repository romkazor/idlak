<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<title>Kaldi: Data types in the &quot;nnet3&quot; setup.</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="favicon.ico" type="image/x-icon" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" /> 
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
 <td id="projectlogo"><a href="http://kaldi-asr.org/"><img alt="Logo" src="KaldiTextAndLogoSmall.png"/ style="padding: 3px 5px 1px 5px"></a></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname" style="display:none">Kaldi
   </div>
  </td>
    <td style="padding-left: 0.5em;">
    <div id="projectbrief" style="display:none"></div>
    </td>
   <!--END PROJECT_BRIEF-->
  <!--END !PROJECT_NAME-->
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('dnn3_code_data_types.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Data types in the "nnet3" setup. </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><ul>
<li>Up: <a class="el" href="dnn3.html">The "nnet3" setup</a></li>
<li>Next: <a class="el" href="dnn3_code_compilation.html">Compilation in the "nnet3" setup</a></li>
</ul>
<h1><a class="anchor" id="dnn3_dt_problem"></a>
Objectives and background</h1>
<p>The previous <a class="el" href="dnn1.html">nnet1</a> and <a class="el" href="dnn2.html">nnet2</a> setups are based on a <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a> object, where a neural net is a stack of Components. Each <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a> corresponds to a layer of the neural net, with the wrinkle that we represent a single layer as an affine transform followed by a nonlinearity, so there are two Components per layer. These old Components had a Propagate function and a Backprop function, both geared towards operating on minibatches, as well as other functions.</p>
<p>Both setups supported more than just a linear sequence of nonlinearities, but in different ways. In the <a class="el" href="dnn1.html">nnet1</a> code, networks with more complex topologies were represented by components-within-components: for instance there was a ParallelComponent, which could contain multiple sequences of Components inside itself. Also, LSTMs were implemented at the C++ level by defining a <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a> to implement the LSTM. In the <a class="el" href="dnn2.html">nnet2</a> code, the network had a notion of a time index to support splicing of features across time as part of the framework directly. This enabled us to support TDNNs by including splicing at intermediate layers of the network.</p>
<p>The objective in the <a class="el" href="namespacekaldi_1_1nnet3.html">nnet3</a> code is to support the kinds of topologies that both the <a class="el" href="dnn1.html">nnet1</a> and <a class="el" href="dnn2.html">nnet2</a> codebases support, and more; and to do so in a natural, config-file-driven way that should not require coding to support most interesting new ideas.</p>
<h1><a class="anchor" id="dnn3_dt_outline"></a>
Outline of approach</h1>
<p>In <a class="el" href="namespacekaldi_1_1nnet3.html">nnet3</a>, instead of just a sequence of Components we have a general graph structure. An "nnet3" neural net (class <a class="el" href="classkaldi_1_1nnet3_1_1Nnet.html">Nnet</a>) consists of:</p><ul>
<li>A list of named Components, in no particular order</li>
<li>A graph structure containing "glue" that specifies how the Components fit together.</li>
</ul>
<p>The graph refers to the Components by name (this enables certain types of parameter sharing). Part of what this "glue" does is to enable things like recurrent neural nets (RNN), where something on time t can depend on time t-1. It also enables us to handle edge effects in a natural way (e.g. the kind of edge effect that happens in an RNN when we reach the beginning of the file).</p>
<p>An example config-file representation of the Components and the graph is given here (we will discuss it in more detail later): </p><pre class="fragment"># First the components
component name=affine1 type=NaturalGradientAffineComponent input-dim=48 output-dim=65
component name=relu1 type=RectifiedLinearComponent dim=65
component name=affine2 type=NaturalGradientAffineComponent input-dim=65 output-dim=115
component name=logsoftmax type=LogSoftmaxComponent dim=115
# Next the nodes
input-node name=input dim=12
component-node name=affine1_node component=affine1 input=Append(Offset(input, -1), Offset(input, 0), Offset(input, 1), Offset(input, 2))
component-node name=nonlin1 component=relu1 input=affine1_node
component-node name=affine2 component=affine2 input=nonlin1
component-node name=output_nonlin component=logsoftmax input=affine2
output-node name=output input=output_nonlin
</pre><p>The graph and the Components, together with the inputs provided and the outputs requested, will be used to construct a "computation graph" (class <a class="el" href="structkaldi_1_1nnet3_1_1ComputationGraph.html" title="The first step in compilation is to turn the ComputationSpecification into a ComputationGraph, where for each Cindex we have a list of other Cindexes that it depends on. ">ComputationGraph</a>), which is an important stage in the compilation process. The computation graph will be an acyclic graph where the nodes corresponded to vector-valued quantities. Each node in that acyclic graph will be identified by the node in the neural network graph (i.e. the layer of the network) together with a number of additional indexes: time (t), an index (n) that indicates the example within the minibatch (e.g. 0 through 511 for a 512-example minibatch), plus an "extra" index (x) that may eventually be useful in convolutional approaches but is usually zero for now.</p>
<p>To formalize the above, we define an <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a> is a tuple (n, t, x). We will also define a <a class="el" href="namespacekaldi_1_1nnet3.html#ac8344e7587df9247d6cc492874901d4d">Cindex</a> as a tuple (node-index, <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a>), where the node-index is the index corresponding to a node in the neural network (i.e., the layer). The actual computation that we create is expressed during compilation as a directed acyclic graph on Cindexes.</p>
<p>The process of using a neural net (whether training or decoding) is as follows:</p><ul>
<li>The user supplies a <a class="el" href="structkaldi_1_1nnet3_1_1ComputationRequest.html">ComputationRequest</a> saying what indexes (e.g. time-indexes) of what inputs are available and what outputs are requested</li>
<li>The <a class="el" href="structkaldi_1_1nnet3_1_1ComputationRequest.html">ComputationRequest</a> together with a neural network is compiled into a sequence of commands as an <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation.html">NnetComputation</a></li>
<li>The <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation.html">NnetComputation</a> is further optimized for speed (think of this is a compiler optimization, like gcc's -O flag).</li>
<li>Class <a class="el" href="classkaldi_1_1nnet3_1_1NnetComputer.html" title="class NnetComputer is responsible for executing the computation described in the &quot;computation&quot; object...">NnetComputer</a> is responsible for receiving matrix-valued input, evaluating the <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation.html">NnetComputation</a>, and supplying matrix-valued output. Think of this as the run-time of a very limited interpreted language.</li>
</ul>
<h1><a class="anchor" id="dnn3_dt_data_structures"></a>
Basic data structures in nnet3</h1>
<h2><a class="anchor" id="dnn3_dt_datastruct_index"></a>
Indexes</h2>
<p>As mentioned above, an <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a> is a tuple (n, t, x), where n is the index within the minibatch, t is the time index, and x is a placeholder index for future use that will usually be zero for now. In the neural net computation there are vector-valued quantities that we deal with (say, a 1024-dimensional quantity corresponding to a hidden layer activation). In the actual neural net computation, 1024 would become the number of columns of a matrix, and there is a one-to-one correspondence between Indexes on the one hand, and rows of the matrix, on the other. The <a class="el" href="namespacekaldi_1_1nnet3.html">nnet3</a> framework thus differs from packages like Theano, where tensor operations are used throughout: instead, we operate efficiently on tensors by packing them into matrices. This enables us to use optimized BLAS operations.</p>
<p>As an example of Indexes: if we are training the very simplest kind of feedforward network, the Indexes would probably only vary in the "n" dimension and we could set the "t" values arbitrarily to zero, so the Indexes would look like </p><pre class="fragment">   [ (0, 0, 0)  (1, 0, 0)  (2, 0, 0) ... ]</pre><p> On the other hand, if we were to decode a single utterance using the same type of network, the Indexes would only vary in the "t" dimension, so we'd have </p><pre class="fragment">   [ (0, 0, 0)  (0, 1, 0)  (0, 2, 0) ... ]</pre><p> corresponding to the rows of the matrix. In a network that uses temporal context, for the early layers we would need different "t" values even during training, so we might encounter lists of Indexes that vary in both "n" and "t", e.g: </p><pre class="fragment">   [ (0, -1, 0)  (0, 0, 0)  (0, 1, 0) (1, -1, 0) (1, 0, 0) (1, 1, 0) ... ]</pre><p> Struct <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a> has a default sorting operator that sorts first by n, then t, then x, so we'd normally order them as above. When you see vectors of Indexes printed out in code you'll often see them in compressed form, where the "x" index (if zero) is omitted, and ranges of "t" values are expressed compactly, so the above vector might be written as </p><pre class="fragment">   [ (0, -1:1) (1, -1:1) ... ]</pre><h2><a class="anchor" id="dnn3_dt_datastruct_cindex"></a>
Cindexes</h2>
<p>A <a class="el" href="namespacekaldi_1_1nnet3.html#ac8344e7587df9247d6cc492874901d4d">Cindex</a> is a pair (int32, <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a>), where the int32 corresponds to the index of a node in a neural network. As mentioned above, a <a class="el" href="classkaldi_1_1nnet3_1_1Nnet.html">neural network</a> consists of a collection of named Components and a kind of graph on "nodes", and the nodes have indexes. Cindexes are used during the compilation process, and they correspond to the nodes of a "computation graph" corresponding to a specific neural net computation. There is a correspondence between a <a class="el" href="namespacekaldi_1_1nnet3.html#ac8344e7587df9247d6cc492874901d4d">Cindex</a> and the output of a particular node, and there will generally (at least, before optimization) be a one-to-one correspondence between Cindexes and rows of matrices in the compiled computation. We previously mentioned that there is a correspondence between Indexes and rows of matrices; the difference is that a Cindex will tell us which matrix, in addition to which row of that matrix. For example, assuming there is a node called "affine1" in the graph, with output dimension 1000 and numbered 2 in the list of nodes, the <a class="el" href="namespacekaldi_1_1nnet3.html#ac8344e7587df9247d6cc492874901d4d">Cindex</a> (2, (0, 0, 0)) would correspond to some row of a matrix of column dimension 1000, that is allocated as the output of the "affine1" component.</p>
<h2><a class="anchor" id="dnn3_dt_datastruct_computation_graph"></a>
ComputationGraph</h2>
<p>A <a class="el" href="structkaldi_1_1nnet3_1_1ComputationGraph.html" title="The first step in compilation is to turn the ComputationSpecification into a ComputationGraph, where for each Cindex we have a list of other Cindexes that it depends on. ">ComputationGraph</a> represents a directed graph on Cindexes, where each Cindex has a list of other Cindexes it depends on. In a simple feedforward structure, the graph will have a simple topology with multiple linear structures where, [using the names not the integers for clarity], we might have (nonlin1, (0, 0, 0)) depending on (affine1, (0, 0, 0)), and (nonlin1, (1, 0, 0)) depending on (affine1, (1, 0, 0)), and so on. In the <a class="el" href="structkaldi_1_1nnet3_1_1ComputationGraph.html" title="The first step in compilation is to turn the ComputationSpecification into a ComputationGraph, where for each Cindex we have a list of other Cindexes that it depends on. ">ComputationGraph</a> and elsewhere you will see integers called cindex_ids. Each cindex_id is an index into an array of Cindexes stored in the graph, and it identifies a particular Cindex; cindex_ids are used for efficiency, as a single integer is easier to work with than a Cindex.</p>
<h2><a class="anchor" id="dnn3_dt_datastruct_computation_request"></a>
ComputationRequest</h2>
<p>A <a class="el" href="structkaldi_1_1nnet3_1_1ComputationRequest.html">ComputationRequest</a> identifies a set of named inputs and output nodes, each with an associated list of <a class="el" href="structkaldi_1_1nnet3_1_1Index.html">Indexes</a>. For input nodes, the list identifies which Indexes are to be provided to the computation; for output nodes, it identifies which Indexes are requested to be computed. In addition the <a class="el" href="structkaldi_1_1nnet3_1_1ComputationRequest.html">ComputationRequest</a> contains various flags, such as information about which output/input nodes have backprop derivatives supplied/requested respectively, and whether model update is to be performed.</p>
<p>As an example, a <a class="el" href="structkaldi_1_1nnet3_1_1ComputationRequest.html">ComputationRequest</a> might specify that there is one input-node, named "input", and with Indexes <code>[ (0, -1, 0), (0, 0, 0), (0, 1, 0) ]</code> provided; and one output-node, named "output", with Indexes <code>[ (0, 0, 0) ]</code> requested. This would make sense if the network required one frame of left and right context. Actually we would typically only request individual output frames like this during training; and during training we would normally have multiple examples in the minibatch so the "n" dimension of the Indexes would vary too.</p>
<p>The computation of the objective function and its derivatives at the output is not part of the core neural network framework; we leave that to the user. A neural network may in general have multiple input and output nodes; this might be useful in multi-task learning or in frameworks which process multiple different types of input data (e.g. multi-view learning).</p>
<h2><a class="anchor" id="dnn3_dt_data_struct_computation"></a>
NnetComputation (brief)</h2>
<p>A <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation.html">NnetComputation</a> represents a specific computation that has been compiled from an <a class="el" href="classkaldi_1_1nnet3_1_1Nnet.html">Nnet</a> and a <a class="el" href="structkaldi_1_1nnet3_1_1ComputationRequest.html">ComputationRequest</a>. It contains a sequence of <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation_1_1Command.html">Commands</a>, each of which could be a Propagate operation, a matrix copy or add operation, various other simple matrix commands such as copying particular rows from one matrix to another; a Backprop operation, matrix sizing commands, and so on. The variables that the Computation acts on are a list of matrices, and also submatrices that may occupy row or column ranges of a matrix. A Computation also contains various sets of indexes (arrays of integers and so on) that are sometimes required as arguments to particular matrix operations.</p>
<p>We will describe this in more detail below in <a class="el" href="dnn3_code_data_types.html#dnn3_dt_nnet_computation">NnetComputation (detail)</a>.</p>
<h2><a class="anchor" id="dnn3_dt_data_struct_computer"></a>
NnetComputer</h2>
<p>The <a class="el" href="classkaldi_1_1nnet3_1_1NnetComputer.html" title="class NnetComputer is responsible for executing the computation described in the &quot;computation&quot; object...">NnetComputer</a> object is responsible for actually executing the <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation.html">NnetComputation</a>. This code for this is actually quite simple (chiefly a loop with a switch statement) since most of the complexity happens during compilation and optimization of the <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation.html">NnetComputation</a>.</p>
<h1><a class="anchor" id="dnn3_dt_nnet"></a>
Neural networks in nnet3</h1>
<p>The previous section should have given you a high-level overview of how the framework fits together. In this section we will go into a little more detail on the structure of the neural network itself, and how we glue Components together and express things like dependencies on an input from time t-1.</p>
<h2><a class="anchor" id="dnn3_dt_nnet_component_basics"></a>
Components (the basics)</h2>
<p>A <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a>, in <a class="el" href="namespacekaldi_1_1nnet3.html">nnet3</a>, is an object with Propagate and Backprop functions. It may contain parameters (say, for an affine layer) or it may just implement a fixed nonlinearity, such as a Sigmoid component. The most important part of the interface of a <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a> is as follows: </p><pre class="fragment">class Component {
 public:
  virtual void Propagate(const ComponentPrecomputedIndexes *indexes,
                         const CuMatrixBase&lt;BaseFloat&gt; &amp;in,
                         CuMatrixBase&lt;BaseFloat&gt; *out) const = 0;
  virtual void Backprop(const std::string &amp;debug_info,
                        const ComponentPrecomputedIndexes *indexes,
                        const CuMatrixBase&lt;BaseFloat&gt; &amp;in_value,
                        const CuMatrixBase&lt;BaseFloat&gt; &amp;out_value,
                        const CuMatrixBase&lt;BaseFloat&gt; &amp;out_deriv,
                        Component *to_update, // may be NULL; may be identical
                                              // to "this" or different.
                        CuMatrixBase&lt;BaseFloat&gt; *in_deriv) const = 0;
   ...
};
</pre><p> For now, please ignore the <code>const <a class="el" href="classkaldi_1_1nnet3_1_1ComponentPrecomputedIndexes.html">ComponentPrecomputedIndexes</a> *indexes</code> argument. A particular <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a> will have an input dimension and an output dimension, and it will generally transform the data "row-by-row". That is, the in and out matrices in Propagate() have the same number of rows, and each row of the input is processed to create the corresponding row of the output. In terms of Indexes, this means that the Indexes corresponding to each element of input and output are the same. Similar logic holds in the Backprop function.</p>
<h2><a class="anchor" id="dnn3_dt_nnet_component_properties"></a>
Components (properties)</h2>
<p>A <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a> has a virtual function "Properties()" that will return a bitmask value containing various binary flags defined as enum <a class="el" href="namespacekaldi_1_1nnet3.html#a36da016976f88292c0d31653a20cbfe3">ComponentProperties</a>. </p><pre class="fragment">class Component {
  ...
  virtual int32 Properties() const = 0;
  ...
};
</pre><p> These properties identify various characteristics of the component, such as whether it contains updatable parameters (kUpdatableComponent), whether its propagate function supports in-place operation (kPropagateInPlace), and various other things. Many of these are needed by the optimization code so it can know which optimizations are applicable. You'll also notice an enum value kSimpleComponent. If set, then the <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a> is "simple" which means it transforms the data row-by-row as defined above. Non-simple Components may allow inputs and outputs with different numbers of rows, and may need to know what indexes are used at the input and output. The <code>const <a class="el" href="classkaldi_1_1nnet3_1_1ComponentPrecomputedIndexes.html">ComponentPrecomputedIndexes</a> *indexes</code> argument to Propgate and Backprop is only for use by non-simple Components. For now, please assume all Components are simple, because we have not implemented any non-simple Components yet, and because they are not required for implementing any of the standard methods (RNNs, LSTMs and so on). Unlike in the <a class="el" href="dnn2.html">nnet2</a> framework, Components are not responsible for implementing things like splicing across frames; instead we use <a class="el" href="dnn3_code_data_types.html#dnn3_dt_nnet_descriptor_code">Descriptors</a> to handle that, as will be explained below.</p>
<h2><a class="anchor" id="dnn3_dt_nnet_node_outline"></a>
Neural network nodes (outline)</h2>
<p>We previously explained that a neural net is a collection of named Components together with a graph on "network nodes", but we haven't yet explained what a "network node" is. <a class="el" href="structkaldi_1_1nnet3_1_1NetworkNode.html" title="NetworkNode is used to represent, three types of thing: either an input of the network (which pretty ...">NetworkNode</a> is actually a struct. A <a class="el" href="structkaldi_1_1nnet3_1_1NetworkNode.html" title="NetworkNode is used to represent, three types of thing: either an input of the network (which pretty ...">NetworkNode</a> may be one of four different varieties, defined by the <a class="el" href="namespacekaldi_1_1nnet3.html#acac9cbaeea226ed297804c012dc12b16">NodeType</a> enum: </p><pre class="fragment">enum NodeType { kInput, kDescriptor, kComponent, kDimRange };
</pre><p> The three most important ones are kInput, kDescriptor and kComponent (kDimRange is included to support splitting up a node's output into various parts). The kComponent nodes are the "meat" of the network, and (at the risk of mixing metaphors) the Descriptors are the "glue" that holds it together, supporting things like frame splicing and recurrence. The kInput nodes are very simple and just provide a place to dump the provided input and to declare its dimension; they don't really do anything. You may be surprised that there is no kOutput node. The reason is that output nodes are simply Descriptors. There is a rule that each node of type kComponent must be immediately preceded in the list of nodes, by its "own" node of type kDescriptor; this rule makes the graph compilation easier. Thus, a node of type kDescriptor that is not immediately followed by a kComponent node is bound to be an output node; for convenience, class <a class="el" href="classkaldi_1_1nnet3_1_1Nnet.html">Nnet</a> has functions <a class="el" href="classkaldi_1_1nnet3_1_1Nnet.html#a6c50812e1be80ab06573493276ec2ca5">IsOutputNode(int32 node_index)</a> and <a class="el" href="classkaldi_1_1nnet3_1_1Nnet.html#ace963c54d5f404914f1ce1953a9112c6">IsComponentInputNode(int32 node_index)</a> that can tell these apart.</p>
<p>We will go into neural network nodes in more detail below in <a class="el" href="dnn3_code_data_types.html#dnn3_dt_nnet_node_detail">Neural network nodes (detail)</a>.</p>
<h2><a class="anchor" id="dnn3_dt_nnet_config"></a>
Neural network config files</h2>
<p>Neural networks can be created from configuration files. We give a very simple example here to show how the configuration files relate to the Descriptors. This network has one hidden layer and does splicing over time in the first node: </p><pre class="fragment"># First the components
component name=affine1 type=NaturalGradientAffineComponent input-dim=48 output-dim=65
component name=relu1 type=RectifiedLinearComponent dim=65
component name=affine2 type=NaturalGradientAffineComponent input-dim=65 output-dim=115
component name=logsoftmax type=LogSoftmaxComponent dim=115
# Next the nodes
input-node name=input dim=12
component-node name=affine1_node component=affine1 input=Append(Offset(input, -1), Offset(input, 0), Offset(input, 1), Offset(input, 2))
component-node name=nonlin1 component=relu1 input=affine1_node
component-node name=affine2 component=affine2 input=nonlin1
component-node name=output_nonlin component=logsoftmax input=affine2
output-node name=output input=output_nonlin
</pre><p> In the config file there is no reference to descriptors (e.g. no "descriptor-node"). Instead, the "input" field (e.g. <code><a class="el" href="classkaldi_1_1Input.html">Input</a>=Append(....)</code>) is the descriptor. Each component-node in the config file gets expanded to two nodes: a node of type kComponent, and an immediately preceding node of type kDescriptor that is defined by the "input" field.</p>
<p>The config file above doesn't give an example of a dim-range node. The basic format of a dim-range node is this (this example would take the first 50 dimensions from the 65 dimensions of component affine1): </p><pre class="fragment">dim-range-node name=dim-range-node1 input-node=affine1_node dim-offset=0 dim=50
</pre><h2><a class="anchor" id="dnn3_dt_nnet_descriptor_config"></a>
Descriptors in config files</h2>
<p>A <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html">Descriptor</a> is a very limited type of expression that refers to quantities defined in other nodes in the graph. Descriptors are part of the glue that attaches components together they are responsible for things like appending or summing the outputs of components so that they can be used as the input for later components. In this section we describe Descriptors from the perspective of their config file format; below we'll explain how they appear in code.</p>
<p>The simplest type of <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html">Descriptor</a> (the base-case) is just a node name, e.g. "affine1" (only nodes of type kComponent or kInput are allowed to appear here, to simplify implementation). We will list below some types of expression that may appear in Descriptors, but please bear in mind that this description will give you a picture of Descriptors that is a little more general than the reality; in reality these may only appear in a certain hierarchy, which we will describe more precisely further down this page. </p><pre class="fragment"># caution, this is a simplification that overgenerates descriptors.
&lt;descriptor&gt;  ::=   &lt;node-name&gt;      ;; node name of kInput or kComponent node.
&lt;descriptor&gt;  ::=   Append(&lt;descriptor&gt;, &lt;descriptor&gt; [, &lt;descriptor&gt; ... ] )
&lt;descriptor&gt;  ::=   Sum(&lt;descriptor&gt;, &lt;descriptor&gt;)
&lt;descriptor&gt;  ::=   Const(&lt;value&gt;, &lt;dimension&gt;)    ;; e.g. Const(1.0, 512)
&lt;descriptor&gt;  ::=   Scale(&lt;scale&gt;, &lt;descriptor&gt;)   ;; e.g. Scale(-1.0, tdnn2)
;; Failover or IfDefined might be useful for time t=-1 in a RNN, for instance.
&lt;descriptor&gt;  ::=   Failover(&lt;descriptor&gt;, &lt;descriptor&gt;)   ;; 1st arg if computable, else 2nd
&lt;descriptor&gt;  ::=   IfDefined(&lt;descriptor&gt;)     ;; the arg if defined, else zero.
&lt;descriptor&gt;  ::=   Offset(&lt;descriptor&gt;, &lt;t-offset&gt; [, &lt;x-offset&gt; ] ) ;; offsets are integers
;; Switch(...) is intended to be used in clockwork RNNs or similar schemes.  It chooses
;; one argument based on the value of t (in the requested Index) modulo the number of
;; arguments
&lt;descriptor&gt;  ::=   Switch(&lt;descriptor&gt;, &lt;descriptor&gt; [, &lt;descriptor&gt; ...])
;; For use in clockwork RNNs or similar, Round() rounds the time-index t of the
;; requested Index to the next-lowest multiple of the integer &lt;t-modulus&gt;,
;; and evaluates the input argument for the resulting Index.
&lt;descriptor&gt;  ::=   Round(&lt;descriptor&gt;, &lt;t-modulus&gt;)  ;; &lt;t-modulus&gt; is an integer
;; ReplaceIndex replaces some &lt;variable-name&gt; (t or x) in the requested Index
;; with a fixed integer &lt;value&gt;.  E.g. might be useful when incorporating
;; iVectors; iVector would always have time-index t=0.
&lt;descriptor&gt;  ::=   ReplaceIndex(&lt;descriptor&gt;, &lt;variable-name&gt;, &lt;value&gt;)
</pre><p>Now we will describe the actual syntax which the code uses internally, which differs from the above simplified version because expressions may appear only in a certain hierarchy. This syntax also corresponds more closely with the class names in the real code. The code that reads Descriptors attempts to normalize them in as general as possible a way, so that almost all of the above syntax can be read and converted to the internal representation. </p><pre class="fragment">;;; &lt;descriptor&gt; == class Descriptor
&lt;descriptor&gt; ::=  Append(&lt;sum-descriptor&gt;[, &lt;sum-descriptor&gt; ... ] )
&lt;descriptor&gt; ::=  &lt;sum-descriptor&gt;  ;; equivalent to Append() with one arg.
;;; &lt;sum-descriptor&gt; == class SumDescriptor
&lt;sum-descriptor&gt; ::= Sum(&lt;sum-descriptor&gt;, &lt;sum-descriptor&gt;)
&lt;sum-descriptor&gt; ::= Failover(&lt;sum-descriptor&gt;, &lt;sum-descriptor&gt;)
&lt;sum-descriptor&gt; ::= IfDefined(&lt;sum-descriptor&gt;)
&lt;sum-descriptor&gt; ::= Const(&lt;value&gt;, &lt;dimension&gt;)
&lt;sum-descriptor&gt; ::= &lt;fwd-descriptor&gt;
;;; &lt;fwd-descriptor&gt; == class ForwardingDescriptor
;; &lt;t-offset&gt; and &lt;x-offset&gt; are integers.
&lt;fwd-descriptor&gt;  ::=   Offset(&lt;fwd-descriptor&gt;, &lt;t-offset&gt; [, &lt;x-offset&gt; ] )
&lt;fwd-descriptor&gt;  ::=   Switch(&lt;fwd-descriptor&gt;, &lt;fwd-descriptor&gt; [, &lt;fwd-descriptor&gt; ...])
;; &lt;t-modulus&gt; is an integer
&lt;fwd-descriptor&gt;  ::=   Round(&lt;fwd-descriptor&gt;, &lt;t-modulus&gt;)
;; &lt;variable-name&gt; is t or x; &lt;value&gt; is an integer
&lt;fwd-descriptor&gt;  ::=   ReplaceIndex(&lt;fwd-descriptor&gt;, &lt;variable-name&gt;, &lt;value&gt;)
;; &lt;node-name&gt; is the name of a node of type kInput or kComponent.
&lt;fwd-descriptor&gt;  ::=   Scale(&lt;scale&gt;, &lt;node-name&gt;)
&lt;fwd-descriptor&gt;  ::=   &lt;node-name&gt;
</pre><p> The design of the Descriptors is supposed to be restrictive enough that the resulting expressions will be fairly easy to compute (and to produce backprop code for). They are only supposed to do heavy lifting when it comes to connecting Components together, while any more interesting or nonlinear operations are supposed to be carried out in the Components themselves.</p>
<p>Note: if it ever becomes necessary to do a sum or average over a variety of indexes of unknown length (e.g. all the "t" values in a file), we intend to do this in a <a class="el" href="classkaldi_1_1nnet3_1_1Component.html" title="Abstract base-class for neural-net components. ">Component</a> - a non-simple Component- rather than using Descriptors.</p>
<h2><a class="anchor" id="dnn3_dt_nnet_descriptor_code"></a>
Descriptors in code</h2>
<p>We'll describe Descriptors in code from the bottom up. The base-class <a class="el" href="classkaldi_1_1nnet3_1_1ForwardingDescriptor.html" title="A ForwardingDescriptor describes how we copy data from another NetworkNode, or from multiple other Ne...">ForwardingDescriptor</a> handles the types of <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html">Descriptor</a> that will reference just a single value, without any <code>Append(...)</code> or <code>Sum(...)</code> expressions or the like. The most important function in this interface is <a class="el" href="classkaldi_1_1nnet3_1_1ForwardingDescriptor.html#a6b70c68dba2537eabbf3768ca945ec0a">MapToInput()</a>: </p><pre class="fragment">class ForwardingDescriptor {
 public:
  virtual Cindex MapToInput(const Index &amp;output) const = 0;
  ...
 }
</pre><p> Given a particular requested <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a>, this function will return a <a class="el" href="namespacekaldi_1_1nnet3.html#ac8344e7587df9247d6cc492874901d4d">Cindex</a> (referencing some other node) corresponding to the input value. The function argument is an <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a> rather than a <a class="el" href="namespacekaldi_1_1nnet3.html#ac8344e7587df9247d6cc492874901d4d">Cindex</a> because the value is never going to depend on the node-index of the node corresponding to the <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html">Descriptor</a> itself. There are several derived classes of <a class="el" href="classkaldi_1_1nnet3_1_1ForwardingDescriptor.html" title="A ForwardingDescriptor describes how we copy data from another NetworkNode, or from multiple other Ne...">ForwardingDescriptor</a>, including <a class="el" href="classkaldi_1_1nnet3_1_1SimpleForwardingDescriptor.html" title="SimpleForwardingDescriptor is the base-case of ForwardingDescriptor, consisting of a source node in t...">SimpleForwardingDescriptor</a> (the base-case, holding just a node index), <a class="el" href="classkaldi_1_1nnet3_1_1OffsetForwardingDescriptor.html" title="Offsets in &#39;t&#39; and &#39;x&#39; values of other ForwardingDescriptors. ">OffsetForwardingDescriptor</a>, <a class="el" href="classkaldi_1_1nnet3_1_1ReplaceIndexForwardingDescriptor.html" title="This ForwardingDescriptor modifies the indexes (n, t, x) by replacing one of them (normally t) with a...">ReplaceIndexForwardingDescriptor</a>, and so on.</p>
<p>The next level up the hierarchy is class <a class="el" href="classkaldi_1_1nnet3_1_1SumDescriptor.html" title="This is an abstract base-class. ">SumDescriptor</a>, which exists to support the expressions <code>Sum(&lt;desc&gt;, &lt;desc&gt;)</code>, <code>Failover(&lt;desc&gt;, &lt;desc&gt;)</code>, and <code>IfDefined(&lt;desc&gt;)</code>. Clearly a request for a given <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a> to a <a class="el" href="classkaldi_1_1nnet3_1_1SumDescriptor.html" title="This is an abstract base-class. ">SumDescriptor</a> may return several different Cindexes, so the interface we used for <a class="el" href="classkaldi_1_1nnet3_1_1ForwardingDescriptor.html" title="A ForwardingDescriptor describes how we copy data from another NetworkNode, or from multiple other Ne...">ForwardingDescriptor</a> won't work. We also need to support optional dependencies. Here is how we manage it at the code level: </p><pre class="fragment">class SumDescriptor {
 public:
  virtual void GetDependencies(const Index &amp;ind,
                               std::vector&lt;Cindex&gt; *dependencies) const = 0;
  ...
};
</pre><p> The function GetDependencies appends to <code>dependencies</code> all Cindexes that are potentially involved in computing this quantity for this <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a>. Next we need to worry about what happens when some of the requested inputs may not be computable (e.g. because of limited input data or edge effects). The function <a class="el" href="classkaldi_1_1nnet3_1_1SumDescriptor.html#aef13aa43e8fb467383121c9d7b71ff1a">IsComputable()</a> handles this: </p><pre class="fragment">class SumDescriptor {
 public:
  ...
  virtual bool IsComputable(const Index &amp;ind,
                            const CindexSet &amp;cindex_set,
                            std::vector&lt;Cindex&gt; *input_terms) const = 0;
  ...
};
</pre><p> Here, the <code><a class="el" href="classkaldi_1_1nnet3_1_1CindexSet.html">CindexSet</a></code> object is a representation of a set of Cindexes, which in this context represents "the set of all Cindexes that we know are computable". If the <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html">Descriptor</a> is computable for this <a class="el" href="structkaldi_1_1nnet3_1_1Index.html" title="struct Index is intended to represent the various indexes by which we number the rows of the matrices...">Index</a>, the function will return true. For instance, the expression <code>Sum(X, Y)</code> would only be computable if <code>X</code> and <code>Y</code> are computable. If this function is going to return true, it will also append to "input_terms" only the input Cindexes that actually appear in the evaluated expression. For example (and speaking loosely), in an expression of the form <code>Failover(X, Y)</code>, if <code>X</code> is computable then only <code>X</code> would be appended to "input_terms", and not <code>Y</code>.</p>
<p>Class <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html">Descriptor</a> is the top level of the hierarchy. It can be thought of as a vector of SumDescriptors, and note that this vector will usually be of length one. Its function is to append things (think of appending vectors), and it is responsible for the <code>Append(...)</code> syntax. It has functions <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html#a06c7efb3516b383368169ff93da74777">GetDependencies()</a> and <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html#a69d01ab492a91282f4542f6304880e02">IsComputable()</a> with the same interface as <a class="el" href="classkaldi_1_1nnet3_1_1SumDescriptor.html" title="This is an abstract base-class. ">SumDescriptor</a>, and also functions such as <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html#adad74bafb94f1407821cf2a5eac2ab3e">NumParts()</a> and <a class="el" href="classkaldi_1_1nnet3_1_1Descriptor.html#a52c269dc9630c51a05c6e0c36e439fd6">Part(int32 n)</a> that allow the user to access the individual SumDescriptors in its vector.</p>
<h2><a class="anchor" id="dnn3_dt_nnet_node_detail"></a>
Neural network nodes (detail)</h2>
<p>We will now describe neural network nodes in more detail. As mentioned above, there are four types of node, as defined by the enum </p><pre class="fragment">enum NodeType { kInput, kDescriptor, kComponent, kDimRange };
</pre><p> The actual <a class="el" href="structkaldi_1_1nnet3_1_1NetworkNode.html" title="NetworkNode is used to represent, three types of thing: either an input of the network (which pretty ...">NetworkNode</a> is a struct. To avoid the hassle of pointers and because C++ doesn't allow unions containing classes, we have a slightly messy layout: </p><pre class="fragment">struct NetworkNode {
  NodeType node_type;
  // "descriptor" is relevant only for nodes of type kDescriptor.
  Descriptor descriptor;
  union {
    // For kComponent, the index into Nnet::components_
    int32 component_index;
    // for kDimRange, the node-index of the input node.
    int32 node_index;
  } u;
  // for kInput, the dimension of the input feature.  For kDimRange, the dimension
  // of the output (i.e. the length of the range)
  int32 dim;
  // for kDimRange, the dimension of the offset into the input component's feature.
  int32 dim_offset;
};
</pre><p> Summarizing the different types of nodes and the members they actually use:</p><ul>
<li>kInput nodes use only "dim"</li>
<li>kDescriptor nodes use only "descriptor"</li>
<li>kComponent nodes use only "component_index", which indexes the components_ array of the <a class="el" href="classkaldi_1_1nnet3_1_1Nnet.html">Nnet</a>.</li>
<li>kDimRange nodes use only "node_index", "dim" and "dim_offset".</li>
</ul>
<h2><a class="anchor" id="dnn3_dt_nnet_detail"></a>
Neural network (detail)</h2>
<p>We will give a little more detail on class <a class="el" href="classkaldi_1_1nnet3_1_1Nnet.html">Nnet</a> itself, which stores the entire neural net. The easiest way to explain it is just to list the private data members: </p><pre class="fragment">class Nnet {
public:
  ...
private:
  std::vector&lt;std::string&gt; component_names_;
  std::vector&lt;Component*&gt; components_;
  std::vector&lt;std::string&gt; node_names_;
  std::vector&lt;NetworkNode&gt; nodes_;

};
</pre><p> The component_names_ should have the same size as components_ and the node_names_ should have the same size as nodes_; these associate names with the components and nodes. Note that we automatically assign names to the nodes of type kDescriptor which precede their corresponding nodes of type kComponent, by appending "_input" to the corresponding component node's name. These names of kDescriptor nodes don't appear in the config-file representation of the neural net.</p>
<h1><a class="anchor" id="dnn3_dt_nnet_computation"></a>
NnetComputation (detail)</h1>
<p>Another important data type is struct <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation.html">NnetComputation</a>. This represents a compiled neural-net computation, containing a sequence of commands together with other information necessary to interpret them. Internally it defines a number of types, including the following enum value: </p><pre class="fragment">  enum CommandType {
    kAllocMatrixUndefined, kAllocMatrixZeroed,
    kDeallocMatrix, kPropagate, kStoreStats, kBackprop,
    kMatrixCopy, kMatrixAdd, kCopyRows, kAddRows,
    kCopyRowsMulti, kCopyToRowsMulti, kAddRowsMulti, kAddToRowsMulti,
    kAddRowRanges, kNoOperation, kNoOperationMarker };
</pre><p> We would like to highlight <code>kPropagate</code>, <code>kBackprop</code> and <code>kMatrixCopy</code> as self-explanatory examples of commands. There is a struct Command which represents a single command together with its arguments. Most of the arguments are indexes into lists of matrices and components. </p><pre class="fragment">  struct Command {
    CommandType command_type;
    int32 arg1;
    int32 arg2;
    int32 arg3;
    int32 arg4;
    int32 arg5;
    int32 arg6;
  };
</pre><p> There are also a couple of struct types defined, which are used to store size information for matrices and submatrices. A submatrix is a possibly restricted row and column range of a matrix, like the matlab syntax <code>some_matrix(1:10, 1:20)</code>: </p><pre class="fragment">  struct MatrixInfo {
    int32 num_rows;
    int32 num_cols;
  };
  struct SubMatrixInfo {
    int32 matrix_index;  // index into "matrices": the underlying matrix.
    int32 row_offset;
    int32 num_rows;
    int32 col_offset;
    int32 num_cols;
  };
</pre><p> The data members of struct <a class="el" href="structkaldi_1_1nnet3_1_1NnetComputation.html">NnetComputation</a> include the following: </p><pre class="fragment">struct Command {
  ...
  std::vector&lt;Command&gt; commands;
  std::vector&lt;MatrixInfo&gt; matrices;
  std::vector&lt;SubMatrixInfo&gt; submatrices;
  // used in kAddRows, kAddToRows, kCopyRows, kCopyToRows.  contains row-indexes.
  std::vector&lt;std::vector&lt;int32&gt; &gt; indexes;
  // used in kAddRowsMulti, kAddToRowsMulti, kCopyRowsMulti, kCopyToRowsMulti.
  // contains pairs (sub-matrix index, row index)- or (-1,-1) meaning don't
  // do anything for this row.
  std::vector&lt;std::vector&lt;std::pair&lt;int32,int32&gt; &gt; &gt; indexes_multi;
  // Indexes used in kAddRowRanges commands, containing pairs (start-index,
  // end-index)
  std::vector&lt;std::vector&lt;std::pair&lt;int32,int32&gt; &gt; &gt; indexes_ranges;
  // Information about where the values and derivatives of inputs and outputs of
  // the neural net live.
  unordered_map&lt;int32, std::pair&lt;int32, int32&gt; &gt; input_output_info;
  bool need_model_derivative;
  // the following is only used in non-simple Components; ignore for now.
  std::vector&lt;ComponentPrecomputedIndexes*&gt; component_precomputed_indexes;
  ...
};
</pre><p> The vectors with "indexes" in their name are arguments to matrix functions such as CopyRows, AddRows and so on, that need vectors of indexes as input (we will copy these to the GPU card before executing the computation).</p>
<p>More information about the individual commands and the meaning of their arguments can be found here.</p>
<ul>
<li>Up: <a class="el" href="dnn3.html">The "nnet3" setup</a></li>
<li>Next: <a class="el" href="dnn3_code_compilation.html">Compilation in the "nnet3" setup</a>. </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="index.html">Kaldi</a></li><li class="navelem"><a class="el" href="dnn.html">Deep Neural Networks in Kaldi</a></li><li class="navelem"><a class="el" href="dnn3.html">The &quot;nnet3&quot; setup</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
