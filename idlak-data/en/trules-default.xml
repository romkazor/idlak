<?xml version="1.0" encoding="utf-8"?>
<rules xmlns="http://www.cereproc.com/rulesdat" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.cereproc.com/rulesdat rulesdat.xsd ">

  <!-- normal tkeniser behavior is as follows:
       1. Tokenise on white space
       2. Strip initial punctuation
       while valid tokens in whitespace token
       3a. Search for valid tokens and create new tokens if none:
       3b. Strip final punctuation
       3c. if nothing matches delete next character and continue
       continue until we reach the end of the whitespace token 

for example:

hello to.my.friend -> hello to[.] my[.] friend
hello 23,456 times$.yes -> hello 23,456 times $[.] yes

i.e. punctuation characters can be included within tokens but not itially

The tokenisation will have a very big impact on following normalisation rules
Do not alter the tokenisation unless ready to fix all norm rules for that 
language

  -->


  <regex name="whitespace">
    <comment>
      Used by the tokeniser.
      Whitespace characters.
    </comment>
    <exp>
      <![CDATA[^([ \n\t\r]+)]]>
    </exp>
  </regex>

  <regex name="punctuation">
    <comment>
      Used by the tokeniser.
      Strip punctuation of the front and back of tokens.
    </comment>
    <exp>
      <![CDATA[^([()\[\]{}"'!?.,;:|]*)]]>
    </exp>
  </regex>

  <regex name="alpha">
    <comment>
      Used by the tokeniser.
      Also, extend it with the language-specific letters as required.
      This is the legal set of characters present in head words in the
      lexicon.
    </comment>
    <exp>
      <![CDATA[^[a-zA-Z_']+]]>
    </exp>
  </regex>

  <!-- a token pattern will be searched for in token.* order in this file 
       this needs to be greedy (more complex tokens first) --> 

  <regex name="token1">
    <comment>
      It is used by the tokeniser to tokenise these characters separately
    </comment>
    <exp>
      <![CDATA[^([\-+\/\*=$£€¥₩@&%|\\#~^¬])]]>
    </exp>
  </regex>

  <regex name="token2">
    <comment>
      Internally punctuated numbers
    </comment>
    <exp>
      <![CDATA[^([0-9]+[0-9\.,]*[0-9]+(s|m|M)?)]]>
    </exp>
  </regex>

  <regex name="token3">
    <comment>
      Not punctuation
    </comment>
    <exp>
      <![CDATA[^([^()\[\]{}"'!?.,;:|]*)]]>
    </exp>
  </regex>

  <lookup name="downcase">
    <comment>
      Important: Do not delete this table, as it is used by the normaliser
      internally.
      Also, extend it with the language-specific letters.
    </comment>
    <exp>
      <![CDATA[{"A":"a", "B":"b", "C":"c", "D":"d", "E":"e", "F":"f", "G":"g", "H":"h", "I":"i", "J":"j", "K":"k", "L":"l", "M":"m", "N":"n", "O":"o", "P":"p", "Q":"q", "R":"r", "S":"s", "T":"t", "U":"u", "V":"v", "W":"w", "X":"x", "Y":"y", "Z":"z", "Ä":"ä", "Ö":"ö", "Ü":"ü"}]]>
    </exp>
  </lookup>

  <lookup name="convertillegal">
    <comment>
      These characters will be mapped before tokenisation. Basically this is to
      reduce the very large set of UTF chars toa  more manageable set. For
      example English systems would typically ignore accents, UTF can be 
      transformed from composite characters to single characters for some
      languages etc. 
    </comment>
    <exp>
      <![CDATA["À":"A", "Á":"A", "Â":"A", "Ã":"A", "Å":"A", "Æ":"AE", "à":"a", "á":"a", "â":"a", "ã":"a", "å":"a", "æ":"ae", "Ç":"C", "ç":"c", "È":"E", "É":"E", "Ê":"E", "Ë":"E", "è":"e", "é":"e", "ê":"e", "ë":"e", "Ì":"I", "Í":"I", "Î":"I", "Ï":"I", "ì":"i", "í":"i", "î":"i", "ï":"i", "Ñ":"N", "ñ":"n", "Ò":"O", "Ó":"O", "Ô":"O", "Õ":"O", "Ø":"O", "ò":"o", "ó":"o", "ô":"o", "õ":"o", "ø":"o", "Ù":"U", "Ú":"U", "Û":"U", "\u0170":u"Ü", "ù":"u", "ú":"u", "û":"u", "\u0171":u"ü", "Ý":"Y", "ý":"y", "０":"0", "１":"1", "２":"2", "３":"3", "４":"4", "５":"5", "６":"6", "７":"7", "８":"8", "９":"9", "Ａ":"A", "Ｂ":"B", "Ｃ":"C", "Ｄ":"D", "Ｅ":"E", "Ｆ":"F", "Ｇ":"G", "Ｈ":"H", "Ｉ":"I", "Ｊ":"J", "Ｋ":"K", "Ｌ":"L", "Ｍ":"M", "Ｎ":"N", "Ｏ":"O", "Ｐ":"P", "Ｑ":"Q", "Ｒ":"R", "Ｓ":"S", "Ｔ":"T", "Ｕ":"U", "Ｖ":"V", "Ｗ":"W", "Ｘ":"X", "Ｙ":"Y", "Ｚ":"Z", "ａ":"a", "ｂ":"b", "ｃ":"c", "ｄ":"d", "ｅ":"e", "ｆ":"f", "ｇ":"g", "ｈ":"h", "ｉ":"i", "ｊ":"j", "ｋ":"k", "ｌ":"l", "ｍ":"m", "ｎ":"n", "ｏ":"o", "ｐ":"p", "ｑ":"q", "ｒ":"r", "ｓ":"s", "ｔ":"t", "ｕ":"u", "ｖ":"v", "ｗ":"w", "ｘ":"x", "ｙ":"y", "ｚ":"z"]]>
    </exp>
  </lookup>

  <lookup name="utfpunc2ascii">
    <comment>
      These characters will be mapped before tokenisation. Basically this is to
      reduce the very large set of UTF punctuation chars to a more manageable 
      set.
    </comment>
    <exp>
      <![CDATA[{"‘":"'", "’":"'", "‛":"'", '“':'"', '”':'"',"΄":"'", "´":"'", "`":"'", "…":".", "\uFF06":"&", "„":'"', '–':'-', '–':'-', '–':'-', '—':'-', "＇":"'"}]]>
    </exp>
  </lookup>


</rules>